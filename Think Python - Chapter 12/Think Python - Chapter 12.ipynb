{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think Python \n",
    "\n",
    "## Chapter 12 Tuples\n",
    "\n",
    "HTML version can be found [here](http://greenteapress.com/thinkpython2/html/thinkpython2013.html \"Chpt 12\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 12.4 Variable-length argument tuples\n",
    "\n",
    "*As an exercise, write a function called `sum_all` that takes any number of arguments and returns their sum.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_all(*args):\n",
    "    return sum(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_all(1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 12.10 Exercises\n",
    "\n",
    "#### Exercise 1  \n",
    "\n",
    "*Write a function called `most_frequent` that takes a string and prints the letters in decreasing order of frequency. Find text samples from several different languages and see how letter frequency varies between languages. Compare your results with the tables at http://en.wikipedia.org/wiki/Letter_frequencies.*\n",
    "\n",
    "*__One immediate problem with this exercise is that the most frequent character in most texts will be the space character.  It's likely that we won't want any non-alphabetic characters (e.g., punctuation, numbers, etc...) in our analysis, so it's a good idea to include code to remove these characters.  Instead of writing two functions - one for only alphabetic characters and a second for all characters - I decided to include a conditional for both options.  Additionally, I included an option for the user to either print the formatted results, or to return the results in a table. Although this hasn't been covered yet in the book, I added default arguments for both of these conditions.  And as has been the case in earlier exercises, I'm using `print(\"\".format())` for the sake of aesthetics.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(sample, only_alpha = True, return_table = False):\n",
    "    \"\"\"\n",
    "    Tabulates the number of characters in a text and \n",
    "    prints out the characters and their \n",
    "    percentage of the sample in order of frequency.  \n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    sample: text to be analyzed\n",
    "    \n",
    "    only_alpha: determines if non-alphabetic\n",
    "    characters (e.g., punctuation, numbers, and spaces) \n",
    "    are tabulated. Default is True.\n",
    "    \n",
    "    return_table: determines if results are to be \n",
    "    formatted and printed, or returned in a non-formatted\n",
    "    table. Default is False.\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {}\n",
    "\n",
    "    # if we only want to consider alphabetic characters\n",
    "    \n",
    "    if only_alpha:\n",
    "    \n",
    "        for s in sample:\n",
    "            if s.isalpha():\n",
    "                s = s.lower()\n",
    "                d[s] = 1 + d.get(s, 0)\n",
    "            \n",
    "    # if we want to analayze all characters in the text\n",
    "    \n",
    "    else:             \n",
    "        \n",
    "        for s in sample:\n",
    "            s = s.lower()\n",
    "            d[s] = 1 + d.get(s, 0) \n",
    "            \n",
    "    # the next conditional will either return a table, or \n",
    "    # print formatted results\n",
    "    \n",
    "    if return_table:\n",
    "        \n",
    "        t = []\n",
    "        for(y, z) in reversed(sorted(zip(d.values(), d.keys()))):\n",
    "            t.append([z, y/len(sample) * 100])\n",
    "\n",
    "        return t\n",
    "    \n",
    "    else:\n",
    "\n",
    "        for(y, z) in reversed(sorted(zip(d.values(), d.keys()))):\n",
    "            print(\"'{}': {:.3f}%\".format(z, (y/len(sample)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Using the Project Gutenberg text of \"Alice Through the Looking-Glass\" by Lewis Carroll, available [here](https://www.gutenberg.org/files/12/12-0.txt \"Alice Through the Looking-Glass\").  N.B. that Project Gutenberg texts start and end with boilerplate that I removed manually before analysis.  I also had to set the encoding when I opened the text.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'e': 9.397%\n",
      "'t': 7.132%\n",
      "'a': 5.880%\n",
      "'o': 5.384%\n",
      "'i': 5.131%\n",
      "'h': 4.984%\n",
      "'n': 4.959%\n",
      "'s': 4.415%\n",
      "'r': 3.524%\n",
      "'d': 3.463%\n",
      "'l': 3.247%\n",
      "'u': 2.514%\n",
      "'w': 1.878%\n",
      "'g': 1.785%\n",
      "'y': 1.740%\n",
      "'c': 1.522%\n",
      "'m': 1.517%\n",
      "'f': 1.328%\n",
      "'p': 0.996%\n",
      "'b': 0.962%\n",
      "'k': 0.938%\n",
      "'v': 0.606%\n",
      "'q': 0.215%\n",
      "'j': 0.097%\n",
      "'x': 0.091%\n",
      "'z': 0.031%\n"
     ]
    }
   ],
   "source": [
    "alice = open('alice.txt', encoding=\"utf8\").read()\n",
    "most_frequent(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__The same analysis, but with the results in a table.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['e', 9.396908392475074],\n",
       " ['t', 7.131538219643485],\n",
       " ['a', 5.879850290724561],\n",
       " ['o', 5.384107879468002],\n",
       " ['i', 5.131303913528712]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent(alice, return_table = True)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__A table of the same text, now with non-alphabetic characters.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' ', 17.72340779746086],\n",
       " ['e', 9.396908392475074],\n",
       " ['t', 7.131538219643485],\n",
       " ['a', 5.879850290724561],\n",
       " ['o', 5.384107879468002]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent(alice, only_alpha = False, return_table = True)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__I tried a Hungarian text to see how the function would deal with diacritics. The text is \"Az arany ember (2. rész)\" by Mór Jókai, and was also found at [Project Gutenberg](https://www.gutenberg.org/files/56592/56592-0.txt \"Hungarian Text\").__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hungarian = open('hungarian.txt', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'e': 8.122%\n",
      "'a': 7.480%\n",
      "'t': 6.877%\n",
      "'n': 4.897%\n",
      "'l': 4.841%\n",
      "'s': 4.121%\n",
      "'i': 3.604%\n",
      "'o': 3.567%\n",
      "'k': 3.433%\n",
      "'z': 3.382%\n",
      "'m': 3.351%\n",
      "'r': 3.181%\n",
      "'g': 2.976%\n",
      "'á': 2.573%\n",
      "'é': 2.320%\n",
      "'y': 2.011%\n",
      "'d': 1.801%\n",
      "'h': 1.626%\n",
      "'v': 1.570%\n",
      "'b': 1.338%\n",
      "'j': 0.931%\n",
      "'ö': 0.866%\n",
      "'f': 0.788%\n",
      "'u': 0.786%\n",
      "'ő': 0.654%\n",
      "'c': 0.641%\n",
      "'p': 0.627%\n",
      "'ó': 0.613%\n",
      "'ü': 0.397%\n",
      "'í': 0.213%\n",
      "'ú': 0.166%\n",
      "'ű': 0.084%\n",
      "'w': 0.060%\n",
      "'x': 0.007%\n",
      "'q': 0.003%\n",
      "'æ': 0.002%\n",
      "'è': 0.000%\n"
     ]
    }
   ],
   "source": [
    "most_frequent(hungarian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Finally, I tried an Italian text: \"La Gioconda\" by Gabriele d'Annunzio, which is available [here](https://www.gutenberg.org/ebooks/23297.txt.utf-8 \"Italian Text\").__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian = open('italian.txt', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'a': 9.429%\n",
      "'e': 8.504%\n",
      "'i': 7.529%\n",
      "'o': 7.014%\n",
      "'l': 5.580%\n",
      "'n': 5.104%\n",
      "'t': 4.761%\n",
      "'r': 4.558%\n",
      "'s': 4.272%\n",
      "'c': 3.464%\n",
      "'d': 2.752%\n",
      "'u': 2.655%\n",
      "'m': 2.122%\n",
      "'p': 1.777%\n",
      "'v': 1.722%\n",
      "'g': 1.283%\n",
      "'h': 1.022%\n",
      "'b': 0.811%\n",
      "'f': 0.776%\n",
      "'z': 0.468%\n",
      "'q': 0.360%\n",
      "'è': 0.323%\n",
      "'à': 0.202%\n",
      "'ù': 0.125%\n",
      "'ò': 0.092%\n",
      "'ì': 0.075%\n",
      "'ó': 0.005%\n",
      "'é': 0.005%\n",
      "'k': 0.003%\n",
      "'ú': 0.001%\n",
      "'í': 0.001%\n",
      "'æ': 0.001%\n",
      "'º': 0.001%\n",
      "'x': 0.001%\n"
     ]
    }
   ],
   "source": [
    "most_frequent(italian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__The rankings and frequencies are very close to what one can find at the [Wikipedia entry for letter frequencies](https://en.wikipedia.org/wiki/Letter_frequency \"Letter frequency\"), though we have to bear in mind that it would be quite difficult to get reliable results on the basis of only one sample.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2  \n",
    "\n",
    "*More anagrams!*\n",
    "\n",
    "*Write a program that reads a word list from a file (see Section 9.1) and prints all the sets of words that are anagrams.*\n",
    "*Here is an example of what the output might look like:*\n",
    "\n",
    "```\n",
    "['deltas', 'desalt', 'lasted', 'salted', 'slated', 'staled']\n",
    "['retainers', 'ternaries']\n",
    "['generating', 'greatening']\n",
    "['resmelts', 'smelters', 'termless']\n",
    "```\n",
    "\n",
    "*Hint: you might want to build a dictionary that maps from a collection of letters to a list of words that can be spelled with those letters. The question is, how can you represent the collection of letters in a way that can be used as a key?*\n",
    "\n",
    "*Modify the previous program so that it prints the longest list of anagrams first, followed by the second longest, and so on.*\n",
    "\n",
    "*In Scrabble a “bingo” is when you play all seven tiles in your rack, along with a letter on the board, to form an eight-letter word. What collection of 8 letters forms the most possible bingos?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open('words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_letters(string):\n",
    "    \"\"\"\n",
    "    Returns the letters in string as a new string\n",
    "    whose letters are in alphabetical order.\n",
    "    \"\"\"\n",
    "        \n",
    "    return ''.join(sorted(list(string.lower())))\n",
    "\n",
    "def find_anagrams(text):\n",
    "    \"\"\"\n",
    "    Takes a text and returns a list of tuples of anagrams.\n",
    "    First item in the tuple is the number of anagrams formed.\n",
    "    Second item is the letters used.\n",
    "    Third item is the anagrams.\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_dict = {}\n",
    "\n",
    "    for line in text:\n",
    "        orig_word = line.strip()\n",
    "        sorted_word = sort_letters(orig_word)\n",
    "        sorted_dict.setdefault(sorted_word, []).append(orig_word)\n",
    "        \n",
    "    anagrams = []\n",
    "\n",
    "    for k, v in sorted_dict.items():\n",
    "        l = len(v)\n",
    "        if l > 1:\n",
    "            anagrams.append((l, k, v))\n",
    "            \n",
    "    return anagrams          \n",
    "\n",
    "def find_longest_list_anagrams(text):\n",
    "    \"\"\"\n",
    "    Takes a text and returns a list of tuples of anagrams.\n",
    "    First item in the tuple is the number of anagrams formed.\n",
    "    Second item is the letters used.\n",
    "    Third item is the anagrams.\n",
    "    Tuples are listed in decreasing order of frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    anagrams = find_anagrams(text)\n",
    "    longest_list_anagrams = []\n",
    "\n",
    "    for l, k, v  in reversed(sorted(anagrams)):\n",
    "        longest_list_anagrams.append((l, k, v))\n",
    "        \n",
    "    return longest_list_anagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11,\n",
       "  'aeprs',\n",
       "  ['apers',\n",
       "   'asper',\n",
       "   'pares',\n",
       "   'parse',\n",
       "   'pears',\n",
       "   'prase',\n",
       "   'presa',\n",
       "   'rapes',\n",
       "   'reaps',\n",
       "   'spare',\n",
       "   'spear']),\n",
       " (11,\n",
       "  'aelrst',\n",
       "  ['alerts',\n",
       "   'alters',\n",
       "   'artels',\n",
       "   'estral',\n",
       "   'laster',\n",
       "   'ratels',\n",
       "   'salter',\n",
       "   'slater',\n",
       "   'staler',\n",
       "   'stelar',\n",
       "   'talers']),\n",
       " (10,\n",
       "  'aelst',\n",
       "  ['least',\n",
       "   'setal',\n",
       "   'slate',\n",
       "   'stale',\n",
       "   'steal',\n",
       "   'stela',\n",
       "   'taels',\n",
       "   'tales',\n",
       "   'teals',\n",
       "   'tesla']),\n",
       " (9,\n",
       "  'einrst',\n",
       "  ['estrin',\n",
       "   'inerts',\n",
       "   'insert',\n",
       "   'inters',\n",
       "   'niters',\n",
       "   'nitres',\n",
       "   'sinter',\n",
       "   'triens',\n",
       "   'trines']),\n",
       " (9,\n",
       "  'aceprs',\n",
       "  ['capers',\n",
       "   'crapes',\n",
       "   'escarp',\n",
       "   'pacers',\n",
       "   'parsec',\n",
       "   'recaps',\n",
       "   'scrape',\n",
       "   'secpar',\n",
       "   'spacer'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the sake of brevity, only listing first five\n",
    "\n",
    "longest_list_anagrams = find_longest_list_anagrams(fin)\n",
    "longest_list_anagrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_scrabble_bingos(text):\n",
    "    \"\"\"\n",
    "    Takes a text and returns a list with tuples of \n",
    "    eight-letter combinations that can be used to form anagrams.\n",
    "    Tuples are listed in decreasing frequency of anagrams formed.\n",
    "    First item in the tuple is the number of anagrams formed.\n",
    "    Second item is the letters used.\n",
    "    Third item is the anagrams.\n",
    "    \"\"\"\n",
    "    lla = find_longest_list_anagrams(text)\n",
    "    \n",
    "    bingos = []\n",
    "    for l, k, v in lla:\n",
    "        if len(k) == 8:\n",
    "            bingos.append((l, k, v))\n",
    "        \n",
    "    return bingos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reinitialize `fin`\n",
    "\n",
    "fin = open('words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__We actually only need the top result, but I'll show the first five, just as confirmation that the first result is actually the correct result.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7,\n",
       "  'aeginrst',\n",
       "  ['angriest',\n",
       "   'astringe',\n",
       "   'ganister',\n",
       "   'gantries',\n",
       "   'granites',\n",
       "   'ingrates',\n",
       "   'rangiest']),\n",
       " (6,\n",
       "  'aeinprst',\n",
       "  ['painters', 'pantries', 'pertains', 'pinaster', 'pristane', 'repaints']),\n",
       " (6,\n",
       "  'aegilnrt',\n",
       "  ['alerting', 'altering', 'integral', 'relating', 'tanglier', 'triangle']),\n",
       " (6,\n",
       "  'aegilnrs',\n",
       "  ['aligners', 'engrails', 'nargiles', 'realigns', 'signaler', 'slangier']),\n",
       " (6,\n",
       "  'aeegnrst',\n",
       "  ['estrange', 'grantees', 'greatens', 'negaters', 'reagents', 'sergeant'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_scrabble_bingos(fin)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  \n",
    "\n",
    "*Two words form a “metathesis pair” if you can transform one into the other by swapping two letters; for example, “converse” and “conserve”. Write a program that finds all of the metathesis pairs in the dictionary. Hint: don’t test all pairs of words, and don’t test all possible swaps.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__If two words form a \"metathesis pair\", that means they would also be anagrams of each other.  So we can save a tremendous amount of time by only considering words which are already anagrams, instead of all the words in the list.__*\n",
    "\n",
    "*__I created `calculate difference` to calculate by how many letters two words differ.  The function will only give a valid result in the words are the same length.  Although it hasn't been introduced yet in \"Think Python 2e\", I'm using `assert` to make sure this is the case.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reinitialize `fin`\n",
    "\n",
    "fin = open('words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_difference(word1, word2):\n",
    "    \"\"\"\n",
    "    Calculates the number of different characters\n",
    "    in word1 and word2.  Words must be same length.\n",
    "    \"\"\"\n",
    "    assert len(word1) == len(word2), \"Words must be same length.\"\n",
    "    \n",
    "    diff = 0\n",
    "    for x, y in zip(word1, word2):\n",
    "        if x != y:\n",
    "            diff += 1\n",
    "            \n",
    "    return diff\n",
    "\n",
    "def find_metathesis_pairs(text):\n",
    "    \"\"\"\n",
    "    Returns a list of 'metathesis pairs' - words that can\n",
    "    be made into new words by exchanging two letters - \n",
    "    from a text.\n",
    "    \"\"\"\n",
    "\n",
    "    # make anagrams\n",
    "    anagrams = find_anagrams(text)\n",
    "\n",
    "    mps = []\n",
    "\n",
    "    # cycle through the tuples of anagrams and pull out words\n",
    "    # which only differ by two letters\n",
    "    for lng, let, ana in anagrams:\n",
    "        for i in range(len(ana)):\n",
    "            for j in range(i + 1, len(ana)):\n",
    "                if calculate_difference(ana[i], ana[j]) == 2:\n",
    "                    mps.append([ana[i], ana[j]])\n",
    "\n",
    "    return mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metathesis_pairs = find_metathesis_pairs(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maters', 'matres']\n",
      "['decenter', 'decentre']\n",
      "['boss', 'sobs']\n",
      "['deucing', 'educing']\n",
      "['certes', 'terces']\n",
      "['keel', 'leek']\n",
      "['arias', 'raias']\n",
      "['whist', 'whits']\n",
      "['realters', 'relaters']\n",
      "['tarps', 'traps']\n"
     ]
    }
   ],
   "source": [
    "# ten random metathesis pairs:\n",
    "\n",
    "from random import randint\n",
    "\n",
    "for i in range(10):\n",
    "    print(metathesis_pairs[randint(0, len(metathesis_pairs))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4  \n",
    "\n",
    "*Here’s another Car Talk Puzzler (http://www.cartalk.com/content/puzzlers):*\n",
    "\n",
    "> *What is the longest English word, that remains a valid English word, as you remove its letters one at a time?* \n",
    "\n",
    "> *Now, letters can be removed from either end, or the middle, but you can’t rearrange any of the letters. Every time you drop a letter, you wind up with another English word. If you do that, you’re eventually going to wind up with one letter and that too is going to be an English word—one that’s found in the dictionary. I want to know what’s the longest word and how many letters does it have?*\n",
    "\n",
    "> *I’m going to give you a little modest example: Sprite. Ok? You start off with sprite, you take a letter off, one from the interior of the word, take the r away, and we’re left with the word spite, then we take the e off the end, we’re left with spit, we take the s off, we’re left with pit, it, and I.*\n",
    "\n",
    "\n",
    "*Write a program to find all words that can be reduced in this way, and then find the longest one.*\n",
    "\n",
    "*This exercise is a little more challenging than most, so here are some suggestions:*\n",
    "\n",
    "<ol>\n",
    "<li><i>You might want to write a function that takes a word and computes a list of all the words that can be formed by removing one letter. These are the “children” of the word.</i></li>\n",
    "<li><i>Recursively, a word is reducible if any of its children are reducible. As a base case, you can consider the empty string reducible.</i></li>\n",
    "<li><i>The wordlist I provided, `words.txt`, doesn’t contain single letter words. So you might want to add “I”, “a”, and the empty string.</i></li>\n",
    "<li><i>To improve the performance of your program, you might want to memoize the words that are known to be reducible.</i></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__As was the case with all the problems in this chapter, I first came across this problem while I was working my way through [\"Think Julia\"](https://benlauwens.github.io/ThinkJulia.jl/latest/book.html#_exercises_14 \"Think Julia, ex. 12.5\"), and I remember finding the exercise extremely difficult.  Since there is no solution for this problem in \"Think Julia\", I had to consult [Allen Downey's Python code for this problem](http://thinkpython2.com/code/reducible.py \"reducible.py\"); but that was far from straightforward, as Python and Julia handle things such as dictionaries and conditional booleans quite differently, and I recall that I needed quite some time to translate the Python code into working Julia code.__*\n",
    "\n",
    "*__While I remembered how to solve the rest of the problems in this chapter without consulting the code I had written while studying from \"Think Julia\", this exercise is much, much more sophisticated, and I didn't have the patience to solve it from scratch.  Therefore, most of this code is based on code I wrote for the Julia implementation of this problem, which is ultimately heavily based on Allen Downey's original solution.  I believe I found one error in Downey's solution.  The code will still run despite the error; but I still think it's an error, nonetheless.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_dict(text):\n",
    "    \"\"\"\n",
    "    Reads lines from text and \n",
    "    returns a dictionary.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for line in fin:   \n",
    "        d[line.strip().lower()] = None\n",
    "    to_add = [\"i\", \"a\", \"\"]\n",
    "    for ta in to_add:\n",
    "        d[ta] = None\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_children(word, word_dict):\n",
    "    \"\"\"\n",
    "    Returns a list of all valid words in word_dict \n",
    "    that can be formed by removing one letter from word.\n",
    "    \"\"\"\n",
    "    \n",
    "    children = []\n",
    "    l = len(word)\n",
    "    \n",
    "    if l == 1:\n",
    "        children.append(\"\")\n",
    "    \n",
    "    else:\n",
    "        for i in range(l):\n",
    "            if i == 0:\n",
    "                child = (word[1:])\n",
    "            elif i == l:\n",
    "                child = (word[:-1])\n",
    "            else:\n",
    "                child = (word[0:i] + word[i + 1:])\n",
    "                \n",
    "            if child in word_dict:\n",
    "                children.append(child)\n",
    "                \n",
    "    return children\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo = {}\n",
    "memo[\"\"] = [\"\"]\n",
    "\n",
    "def is_reducible(word, word_dict):\n",
    "    \"\"\"\n",
    "    Returns a list of its reducible children if a word \n",
    "    is reducible. A string is reducible if it has at \n",
    "    least one child that is also reducible. The empty \n",
    "    string is considered to be reducible. Adds an entry \n",
    "    to the memo dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    if word in memo:\n",
    "        return memo[word]\n",
    "    \n",
    "    results = []\n",
    "    for child in find_children(word, word_dict):\n",
    "        if is_reducible(child, word_dict):\n",
    "            results.append(child)\n",
    "            \n",
    "    memo[word] = results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_reducible(word_dict):\n",
    "    \"\"\"\n",
    "    Checks all words in word_dict and returns a list of reducible ones.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for word in word_dict:\n",
    "        t = is_reducible(word, word_dict)\n",
    "        if t != []:\n",
    "            results.append(word)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different in Downey's solution.  Error in original?\n",
    "\n",
    "def print_trail(word, word_dict):\n",
    "    \"\"\"\n",
    "    Prints the sequence of words that reduces this word \n",
    "    to the empty string.  Chooses the first if there is \n",
    "    more than one word in the array of reducible words.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(word) == 0:\n",
    "        return\n",
    "    \n",
    "    print(word, end = \" \")\n",
    "    t = is_reducible(word, word_dict)\n",
    "    print_trail(t[0], word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_longest_words(word_dict, n = 5):\n",
    "    \"\"\"\n",
    "    Finds the longest reducible words in word_dict and \n",
    "    prints them and their trails. \n",
    "    \"\"\"\n",
    "    \n",
    "    words = all_reducible(word_dict)\n",
    "    \n",
    "    t = []\n",
    "    for word in words:\n",
    "        t.append((len(word), word))\n",
    "        \n",
    "    t.sort(reverse = True)\n",
    "    \n",
    "    for x, word in t[0:n]:\n",
    "        print_trail(word, word_dict)\n",
    "        print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complecting completing competing compting comping coping oping ping pig pi i \n",
      "\n",
      "twitchiest witchiest withiest withies withes wites wits its is i \n",
      "\n",
      "stranglers strangers stranger strange strang stang tang tag ta a \n",
      "\n",
      "staunchest stanchest stanches stances stanes sanes anes ane ae a \n",
      "\n",
      "restarting restating estating stating sating sting ting tin in i \n",
      "\n",
      "insolating isolating solating slating sating sting ting tin in i \n",
      "\n",
      "completing competing compting comping coping oping ping pig pi i \n",
      "\n",
      "carrousels carousels carouses arouses arouse arose arse are ae a \n",
      "\n",
      "wrappings wrapping rapping raping aping ping pig pi i \n",
      "\n",
      "wranglers wanglers anglers angers agers ages age ae a \n",
      "\n",
      "witchiest withiest withies withes wites wits its is i \n",
      "\n",
      "whittlers whitters whitter whiter white wite wit it i \n",
      "\n",
      "upreaches preaches peaches peaces paces aces ace ae a \n",
      "\n",
      "upreached preached peached peaced paced aced ace ae a \n",
      "\n",
      "upraisers praisers raisers rasers rases rase ras as a \n",
      "\n",
      "twitchier witchier withier wither withe wite wit it i \n",
      "\n",
      "twitchers twitches witches withes wites wits its is i \n",
      "\n",
      "twanglers wanglers anglers angers agers ages age ae a \n",
      "\n",
      "twaddlers waddlers wadders waders wades waes was as a \n",
      "\n",
      "truckling trucking trucing truing ruing ring rin in i \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fin = open('words.txt')\n",
    "my_dict = make_word_dict(fin)\n",
    "print_longest_words(my_dict, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
