{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think Python\n",
    "\n",
    "## Chapter 13 - Case study: data structure selection\n",
    "\n",
    "### 13.1 Word frequency analysis\n",
    "\n",
    "*HTML of this chapter in \"Think Python 2e\" can be found [here](http://greenteapress.com/thinkpython2/html/thinkpython2014.html \"Chapter 13\").*\n",
    "\n",
    "#### Exercise 1  \n",
    "\n",
    "*Write a program that reads a file, breaks each line into words, strips whitespace and punctuation from the words, and converts them to lowercase.*\n",
    "\n",
    "*Hint: The `string` module provides a string named whitespace, which contains space, tab, newline, etc., and `punctuation` which contains the punctuation characters. Let’s see if we can make Python swear:*\n",
    "\n",
    "```\n",
    ">>> import string\n",
    ">>> string.punctuation\n",
    "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "```\n",
    "\n",
    "*Also, you might consider using the string methods `strip`, `replace` and `translate`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punct = string.punctuation\n",
    "\n",
    "# Some PG texts use non-ASCII characters, so we'll add these manually.\n",
    "punct += '‘’“”—'\n",
    "\n",
    "# Some PG texts also have line numbers, so let's remove all numbers:\n",
    "punct += string.digits\n",
    "\n",
    "out = \" \" * len(punct)\n",
    "\n",
    "def get_words_from_file(text, encode = \"utf8\"):\n",
    "    \"\"\"\n",
    "    Returns a list of words from a file.  Words\n",
    "    are stripped of punctuation and converted to lowercase.\n",
    "    Since punctuation is removed, contractions are returned\n",
    "    without apostrophes (e.g., `can't` -> `cant`). Numbers\n",
    "    are also removed from the returned list\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text: name of file\n",
    "    encode: text encoding used in file.  Default is UTF-8\n",
    "    \"\"\"\n",
    "    opened_text = open(text, 'r', encoding = encode)\n",
    "    t = []\n",
    "    for line in opened_text:\n",
    "        translation = line.maketrans(punct, out)\n",
    "        for word in line.translate(translation).split():\n",
    "            t.append(word.strip().lower())\n",
    "            \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\ThinkPython\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['through',\n",
       " 'the',\n",
       " 'looking',\n",
       " 'glass',\n",
       " 'by',\n",
       " 'lewis',\n",
       " 'carroll',\n",
       " 'chapter',\n",
       " 'i',\n",
       " 'looking',\n",
       " 'glass',\n",
       " 'house',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'was',\n",
       " 'certain',\n",
       " 'that',\n",
       " 'the',\n",
       " 'white',\n",
       " 'kitten',\n",
       " 'had',\n",
       " 'had',\n",
       " 'nothing',\n",
       " 'to']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma = get_words_from_file('alice.txt')\n",
    "ma[1:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2  \n",
    "\n",
    "*Go to Project Gutenberg (http://gutenberg.org) and download your favorite out-of-copyright book in plain text format.*\n",
    "\n",
    "*Modify your program from the previous exercise to read the book you downloaded, skip over the header information at the beginning of the file, and process the rest of the words as before.*\n",
    "\n",
    "*Then modify the program to count the total number of words in the book, and the number of times each word is used.*\n",
    "\n",
    "*Print the number of different words used in the book. Compare different books by different authors, written in different eras. Which author uses the most extensive vocabulary?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__I had difficulty removing the boilerplate at the beginning of Project Gutenberg texts, as I felt that the book didn't do an adequate job in showing us how to remove headers and footers.  So I went online to see how other people dealt with this problem.  At [this blog](https://epequeno.wordpress.com/2012/05/06/exercise-13-2/ \"Removing headers from PG texts\") I found some interesting code for removing the header of PG texts; but the creator of the code neglected to deal with the footer, so I made a few changes to deal with that:__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punct = string.punctuation\n",
    "\n",
    "# Some PG texts use non-ASCII quotes, so we'll add these manually.\n",
    "punct += '‘’“”'\n",
    "\n",
    "# Some PG texts also have line numbers, so let's remove all numbers:\n",
    "punct += string.digits\n",
    "\n",
    "out = \" \" * len(punct)\n",
    "\n",
    "def clean_pg_text(text, encode = \"utf8\"):\n",
    "    \"\"\"\n",
    "    Returns a list of words from a Project Gutenberg text.  \n",
    "    Headers and footers are removed from texts. Words\n",
    "    are stripped of punctuation and converted to lowercase.\n",
    "    Since punctuation is removed, contractions are returned\n",
    "    without apostrophes (e.g., `can't` -> `cant`). Numbers\n",
    "    are also removed from the returned list\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text: name of file\n",
    "    encode: text encoding used in file.  Default is UTF-8\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Removes headers and footers from Project Gutenberg texts.\n",
    "    \"\"\"\n",
    "    opened_text = open(text, 'r', encoding = encode)\n",
    "    cleaned_text = []\n",
    "    flag = False\n",
    "    start = \"*** START OF\"\n",
    "    end = \"*** END OF\"\n",
    "\n",
    "    # some PG texts don't use spaces to designate start/end of text\n",
    "    alt_start = \"***START OF\"\n",
    "    alt_end = \"***END OF\"\n",
    "    \n",
    "    for line in opened_text:\n",
    "        if (start in line) or (alt_start in line) and (flag == False):\n",
    "            flag = True\n",
    "        elif (end in line) or (alt_end in line) and (flag == True):\n",
    "            flag = False\n",
    "        elif flag == True:\n",
    "            for line in opened_text:\n",
    "                translation = line.maketrans(punct, out)\n",
    "                for word in line.translate(translation).split():\n",
    "                    cleaned_text.append(word.strip().lower())\n",
    "        else:\n",
    "            pass\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_words(text_list):\n",
    "    \"\"\"\n",
    "    Returns a tally of the words in text_list.\n",
    "    \"\"\"\n",
    "    total_words = {}\n",
    "\n",
    "    for word in text_list:\n",
    "        total_words[word] = 1 + total_words.get(word, 0)\n",
    "\n",
    "    return total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['austen.txt', 'beowulf.txt', 'canterbury.txt', \n",
    "         'hamlet.txt', 'iliad.txt', 'sherlock.txt', 'ulysses.txt']\n",
    "\n",
    "titles = [\"'Pride and Prejudice' by Jane Austen\",\n",
    "          \"'Beowulf' translated by Lesslie Hall\",\n",
    "          \"'The Canterbury Tales' by Geoffrey Chaucer\",\n",
    "          \"'Hamlet' by William Shakespeare\",\n",
    "          \"'The Iliad of Homer', translated by Alexander Pope\",\n",
    "          \"'The Adventures of Sherlock Holmes' by Arthur Conan Doyle\",\n",
    "          \"'Ulysses' by James Joyce\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Pride and Prejudice' by Jane Austen uses 6,525 words\n",
      "'Beowulf' translated by Lesslie Hall uses 5,935 words\n",
      "'The Canterbury Tales' by Geoffrey Chaucer uses 15,494 words\n",
      "'Hamlet' by William Shakespeare uses 5,021 words\n",
      "'The Iliad of Homer', translated by Alexander Pope uses 12,639 words\n",
      "'The Adventures of Sherlock Holmes' by Arthur Conan Doyle uses 8,238 words\n",
      "'Ulysses' by James Joyce uses 29,720 words\n"
     ]
    }
   ],
   "source": [
    "for text, title in zip(texts, titles):\n",
    "    clean = clean_pg_text(text)\n",
    "    tally = tally_words(clean)\n",
    "    print(\"{} uses {:,} words\".format(title, len(tally)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3  \n",
    "\n",
    "*Modify the program from the previous exercise to print the 20 most frequently used words in the book.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_common_words(text, n):\n",
    "    \"\"\"\n",
    "    Returns a list of the n most common words in a \n",
    "    Project Gutenberg text.\n",
    "    \"\"\"\n",
    "    \n",
    "    clean = clean_pg_text(text)\n",
    "    tally = tally_words(clean)\n",
    "    \n",
    "    sorted_words = []\n",
    "    \n",
    "    for (y, z) in reversed(sorted(zip(tally.values(), tally.keys()))):\n",
    "        sorted_words.append([z, y])\n",
    "        \n",
    "    return sorted_words[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 15040],\n",
       " ['of', 8253],\n",
       " ['and', 7217],\n",
       " ['a', 6536],\n",
       " ['to', 5032],\n",
       " ['in', 4995],\n",
       " ['he', 4174],\n",
       " ['his', 3326],\n",
       " ['s', 2837],\n",
       " ['i', 2828],\n",
       " ['that', 2730],\n",
       " ['with', 2556],\n",
       " ['it', 2491],\n",
       " ['was', 2127],\n",
       " ['on', 2125],\n",
       " ['you', 2029],\n",
       " ['for', 1952],\n",
       " ['her', 1784],\n",
       " ['him', 1522],\n",
       " ['is', 1432]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_most_common_words('ulysses.txt', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4  \n",
    "\n",
    "*Modify the previous program to read a word list (see Section 9.1) and then print all the words in the book that are not in the word list. How many of them are typos? How many of them are common words that should be in the word list, and how many of them are really obscure?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from code in ex. 12.4\n",
    "\n",
    "def make_word_dict(text):\n",
    "    \"\"\"\n",
    "    Reads lines from text and \n",
    "    returns a dictionary.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for line in text:   \n",
    "        d[line.strip().lower()] = None\n",
    "    to_add = [\"i\", \"a\"]\n",
    "    for ta in to_add:\n",
    "        d[ta] = None\n",
    "    return d\n",
    "\n",
    "def find_words_not_in_dict(text, t):\n",
    "    \"\"\"\n",
    "    Takes a Project Gutenberg text and a\n",
    "    wordlist and returns a list of words\n",
    "    in the PG text that cannot be found\n",
    "    in the list.\n",
    "    \n",
    "    Arguments:\n",
    "    text: a raw Project Gutenberg text file\n",
    "    t: a raw text word list with one word per line\n",
    "    \"\"\"\n",
    "    check_dict = make_word_dict(t)\n",
    "    clean = clean_pg_text(text)\n",
    "    tally = tally_words(clean)\n",
    "    not_in_dict = []\n",
    "    for word in tally.keys():\n",
    "        if word not in check_dict:\n",
    "            not_in_dict.append(word)\n",
    "\n",
    "    return not_in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen',\n",
       " 'neighbourhood',\n",
       " 'mr',\n",
       " 'netherfield',\n",
       " 'mrs',\n",
       " 'england',\n",
       " 'monday',\n",
       " 'michaelmas',\n",
       " 'bingley',\n",
       " 'william',\n",
       " 'lucas',\n",
       " 'lizzy',\n",
       " 'lydia',\n",
       " 'elizabeth',\n",
       " 't',\n",
       " 's',\n",
       " 'neices',\n",
       " 'mary',\n",
       " 'm',\n",
       " 'neighbour',\n",
       " 'favourable',\n",
       " 'delightful',\n",
       " 'etc',\n",
       " 'hertfordshire',\n",
       " 'london',\n",
       " 'gentlemanlike',\n",
       " 'hurst',\n",
       " 'darcy',\n",
       " 'derbyshire',\n",
       " 'unreserved',\n",
       " 'behaviour',\n",
       " 'fastidious',\n",
       " 'catherine',\n",
       " 'longbourn',\n",
       " 'inhabitants']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = open('words.txt')\n",
    "austen_obscura = find_words_not_in_dict('austen.txt', fin)\n",
    "austen_obscura[:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Most of the words not in the word list are proper names (including single letters that were most likely initials in the original texts) and obscure (or anachronistic) words.  However, in some texts the results were words with punctuation that had not been cleaned by the function `clean_pg_text`.  I therefore went back to the function and added those characters to the list of ones that should be replaced.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
