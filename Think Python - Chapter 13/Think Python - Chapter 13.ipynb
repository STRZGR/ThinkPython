{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think Python\n",
    "\n",
    "## Chapter 13 - Case study: data structure selection\n",
    "\n",
    "### 13.1 Word frequency analysis\n",
    "\n",
    "*HTML of this chapter in \"Think Python 2e\" can be found [here](http://greenteapress.com/thinkpython2/html/thinkpython2014.html \"Chapter 13\").*\n",
    "\n",
    "#### Exercise 1  \n",
    "\n",
    "*Write a program that reads a file, breaks each line into words, strips whitespace and punctuation from the words, and converts them to lowercase.*\n",
    "\n",
    "*Hint: The `string` module provides a string named whitespace, which contains space, tab, newline, etc., and `punctuation` which contains the punctuation characters. Let’s see if we can make Python swear:*\n",
    "\n",
    "```\n",
    ">>> import string\n",
    ">>> string.punctuation\n",
    "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "```\n",
    "\n",
    "*Also, you might consider using the string methods `strip`, `replace` and `translate`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punct = string.punctuation\n",
    "\n",
    "# Some PG texts use non-ASCII characters, so we'll add these manually.\n",
    "punct += '‘’“”—'\n",
    "\n",
    "# Some PG texts also have line numbers, so let's remove all numbers:\n",
    "punct += string.digits\n",
    "\n",
    "out = \" \" * len(punct)\n",
    "\n",
    "def get_words_from_file(text, encode = \"utf8\"):\n",
    "    \"\"\"\n",
    "    Returns a list of words from a file.  Words\n",
    "    are stripped of punctuation and converted to lowercase.\n",
    "    Since punctuation is removed, contractions are returned\n",
    "    without apostrophes (e.g., `can't` -> `can t`). Numbers\n",
    "    are also removed from the returned list\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text: name of file\n",
    "    encode: text encoding used in file.  Default is UTF-8\n",
    "    \"\"\"\n",
    "    opened_text = open(text, 'r', encoding = encode)\n",
    "    t = []\n",
    "    for line in opened_text:\n",
    "        translation = line.maketrans(punct, out)\n",
    "        for word in line.translate(translation).split():\n",
    "            t.append(word.strip().lower())\n",
    "            \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\ThinkPython\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['through',\n",
       " 'the',\n",
       " 'looking',\n",
       " 'glass',\n",
       " 'by',\n",
       " 'lewis',\n",
       " 'carroll',\n",
       " 'chapter',\n",
       " 'i',\n",
       " 'looking',\n",
       " 'glass',\n",
       " 'house',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'was',\n",
       " 'certain',\n",
       " 'that',\n",
       " 'the',\n",
       " 'white',\n",
       " 'kitten',\n",
       " 'had',\n",
       " 'had',\n",
       " 'nothing',\n",
       " 'to']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma = get_words_from_file('alice.txt')\n",
    "ma[1:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2  \n",
    "\n",
    "*Go to Project Gutenberg (http://gutenberg.org) and download your favorite out-of-copyright book in plain text format.*\n",
    "\n",
    "*Modify your program from the previous exercise to read the book you downloaded, skip over the header information at the beginning of the file, and process the rest of the words as before.*\n",
    "\n",
    "*Then modify the program to count the total number of words in the book, and the number of times each word is used.*\n",
    "\n",
    "*Print the number of different words used in the book. Compare different books by different authors, written in different eras. Which author uses the most extensive vocabulary?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__I had difficulty removing the boilerplate at the beginning of Project Gutenberg texts, as I felt that the book didn't do an adequate job in showing us how to remove headers and footers.  So I went online to see how other people dealt with this problem.  At [this blog](https://epequeno.wordpress.com/2012/05/06/exercise-13-2/ \"Removing headers from PG texts\") I found some interesting code for removing the header of PG texts; but the creator of the code neglected to deal with the footer, so I made a few changes to deal with that:__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punct = string.punctuation\n",
    "\n",
    "# Some PG texts use non-ASCII quotes, so we'll add these manually.\n",
    "punct += '‘’“”'\n",
    "\n",
    "# Some PG texts also have line numbers, so let's remove all numbers:\n",
    "punct += string.digits\n",
    "\n",
    "out = \" \" * len(punct)\n",
    "\n",
    "def clean_pg_text(text, encode = \"utf8\"):\n",
    "    \"\"\"\n",
    "    Returns a list of words from a Project Gutenberg text.  \n",
    "    Headers and footers are removed from texts. Words\n",
    "    are stripped of punctuation and converted to lowercase.\n",
    "    Since punctuation is removed, contractions are returned\n",
    "    without apostrophes (e.g., `can't` -> `cant`). Numbers\n",
    "    are also removed from the returned list\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text: name of file\n",
    "    encode: text encoding used in file.  Default is UTF-8\n",
    "    \"\"\"\n",
    "    opened_text = open(text, 'r', encoding = encode)\n",
    "    cleaned_text = []\n",
    "    flag = False\n",
    "    start = \"*** START OF\"\n",
    "    end = \"*** END OF\"\n",
    "\n",
    "    # some PG texts don't use spaces to designate start/end of text\n",
    "    alt_start = \"***START OF\"\n",
    "    alt_end = \"***END OF\"\n",
    "    \n",
    "    for line in opened_text:\n",
    "        if ((start in line) or (alt_start in line)) and flag == False:\n",
    "            flag = True\n",
    "        elif ((end in line) or (alt_end in line)) and flag == True:\n",
    "            return cleaned_text\n",
    "        elif flag == True:\n",
    "                translation = line.maketrans(punct, out)\n",
    "                for word in line.translate(translation).split():\n",
    "                    cleaned_text.append(word.strip().lower())\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_words(text_list):\n",
    "    \"\"\"\n",
    "    Returns a tally of the words in text_list.\n",
    "    \"\"\"\n",
    "    total_words = {}\n",
    "\n",
    "    for word in text_list:\n",
    "        total_words[word] = 1 + total_words.get(word, 0)\n",
    "\n",
    "    return total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['austen.txt', 'beowulf.txt', 'canterbury.txt', \n",
    "         'hamlet.txt', 'iliad.txt', 'sherlock.txt', 'ulysses.txt']\n",
    "\n",
    "titles = [\"'Pride and Prejudice' by Jane Austen\",\n",
    "          \"'Beowulf' translated by Lesslie Hall\",\n",
    "          \"'The Canterbury Tales' by Geoffrey Chaucer\",\n",
    "          \"'Hamlet' by William Shakespeare\",\n",
    "          \"'The Iliad of Homer', translated by Alexander Pope\",\n",
    "          \"'The Adventures of Sherlock Holmes' by Arthur Conan Doyle\",\n",
    "          \"'Ulysses' by James Joyce\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Pride and Prejudice' by Jane Austen uses 6,263 words\n",
      "'Beowulf' translated by Lesslie Hall uses 5,607 words\n",
      "'The Canterbury Tales' by Geoffrey Chaucer uses 15,135 words\n",
      "'Hamlet' by William Shakespeare uses 4,642 words\n",
      "'The Iliad of Homer', translated by Alexander Pope uses 12,400 words\n",
      "'The Adventures of Sherlock Holmes' by Arthur Conan Doyle uses 7,986 words\n",
      "'Ulysses' by James Joyce uses 29,575 words\n"
     ]
    }
   ],
   "source": [
    "for text, title in zip(texts, titles):\n",
    "    clean = clean_pg_text(text)\n",
    "    tally = tally_words(clean)\n",
    "    print(\"{} uses {:,} words\".format(title, len(tally)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3  \n",
    "\n",
    "*Modify the program from the previous exercise to print the 20 most frequently used words in the book.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_common_words(text, n):\n",
    "    \"\"\"\n",
    "    Returns a list of the n most common words in a \n",
    "    Project Gutenberg text.\n",
    "    \"\"\"\n",
    "    \n",
    "    clean = clean_pg_text(text)\n",
    "    tally = tally_words(clean)\n",
    "    \n",
    "    sorted_words = []\n",
    "    \n",
    "    for (y, z) in reversed(sorted(zip(tally.values(), tally.keys()))):\n",
    "        sorted_words.append([z, y])\n",
    "        \n",
    "    return sorted_words[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 14863],\n",
       " ['of', 8138],\n",
       " ['and', 7148],\n",
       " ['a', 6478],\n",
       " ['to', 4951],\n",
       " ['in', 4935],\n",
       " ['he', 4171],\n",
       " ['his', 3326],\n",
       " ['i', 2828],\n",
       " ['s', 2823],\n",
       " ['that', 2715],\n",
       " ['with', 2510],\n",
       " ['it', 2479],\n",
       " ['was', 2125],\n",
       " ['on', 2114],\n",
       " ['you', 1957],\n",
       " ['for', 1927],\n",
       " ['her', 1784],\n",
       " ['him', 1522],\n",
       " ['is', 1409]]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_most_common_words('ulysses.txt', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4  \n",
    "\n",
    "*Modify the previous program to read a word list (see Section 9.1) and then print all the words in the book that are not in the word list. How many of them are typos? How many of them are common words that should be in the word list, and how many of them are really obscure?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from code in ex. 12.4\n",
    "\n",
    "def make_word_dict(text):\n",
    "    \"\"\"\n",
    "    Reads lines from text and \n",
    "    returns a dictionary.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for line in text:   \n",
    "        d[line.strip().lower()] = None\n",
    "    to_add = [\"i\", \"a\"]\n",
    "    for ta in to_add:\n",
    "        d[ta] = None\n",
    "    return d\n",
    "\n",
    "def find_words_not_in_dict(text, t):\n",
    "    \"\"\"\n",
    "    Takes a Project Gutenberg text and a\n",
    "    wordlist and returns a list of words\n",
    "    in the PG text that cannot be found\n",
    "    in the list.\n",
    "    \n",
    "    Arguments:\n",
    "    text: a raw Project Gutenberg text file\n",
    "    t: a raw text word list with one word per line\n",
    "    \"\"\"\n",
    "    check_dict = make_word_dict(t)\n",
    "    clean = clean_pg_text(text)\n",
    "    tally = tally_words(clean)\n",
    "    not_in_dict = []\n",
    "    for word in tally.keys():\n",
    "        if word not in check_dict:\n",
    "            not_in_dict.append(word)\n",
    "\n",
    "    return not_in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen',\n",
       " 'neighbourhood',\n",
       " 'mr',\n",
       " 'netherfield',\n",
       " 'mrs',\n",
       " 'england',\n",
       " 'monday',\n",
       " 'michaelmas',\n",
       " 'bingley',\n",
       " 'william',\n",
       " 'lucas',\n",
       " 'lizzy',\n",
       " 'lydia',\n",
       " 'elizabeth',\n",
       " 't',\n",
       " 's',\n",
       " 'neices',\n",
       " 'mary',\n",
       " 'm',\n",
       " 'neighbour',\n",
       " 'favourable',\n",
       " 'delightful',\n",
       " 'etc',\n",
       " 'hertfordshire',\n",
       " 'london',\n",
       " 'gentlemanlike',\n",
       " 'hurst',\n",
       " 'darcy',\n",
       " 'derbyshire',\n",
       " 'unreserved',\n",
       " 'behaviour',\n",
       " 'fastidious',\n",
       " 'catherine',\n",
       " 'longbourn',\n",
       " 'inhabitants']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = open('words.txt')\n",
    "austen_obscura = find_words_not_in_dict('austen.txt', fin)\n",
    "austen_obscura[:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Most of the words not in the word list are proper names.  A number of British spellings also make an appearance, as well as some words that seem common enough that their absence in the wordlist is a bit surprising (e.g., 'monday', 'delightful', 'neices', etc...). I believe the word list was compiled for crossword answers, so this fact may explain their absence. Also included are what are likely 'contraction orphans' - orphaned letters created when apostrophes are removed from contractions.  However, in some texts the results were words with punctuation that had not been cleaned by the function `clean_pg_text`.  I therefore went back to the function and added those characters to the list of ones that should be replaced.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5  \n",
    "\n",
    "*Write a function named `choose_from_hist` that takes a histogram as defined in Section 11.2 and returns a random value from the histogram, chosen with probability in proportion to frequency. For example, for this histogram:*\n",
    "\n",
    "```\n",
    ">>> t = ['a', 'a', 'b']\n",
    ">>> hist = histogram(t)\n",
    ">>> hist\n",
    "{'a': 2, 'b': 1}\n",
    "```\n",
    "\n",
    "*your function should return 'a' with probability 2/3 and 'b' with probability 1/3.*\n",
    "\n",
    "*__IMHO, this function is somewhat naff: all the function `choose_from_hist` does is recreate the original list `t` from the histogram `h`, and then randomly chooses an element from the list. Couldn't we just skip a few steps and use `random.choose()` on the original list?__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# from chapter 11\n",
    "\n",
    "def histogram(s):\n",
    "    d = dict()\n",
    "    for c in s:\n",
    "        d[c] = 1 + d.get(c, 0)\n",
    "\n",
    "    return d\n",
    "\n",
    "def choose_from_hist(h):\n",
    "    \"\"\"\n",
    "    Returns a random item from histogram h.\n",
    "    Probability item will be chosen is based on \n",
    "    frequency within the histogram. \n",
    "    \"\"\"\n",
    "    \n",
    "    t = []\n",
    "    for k, v in h.items():\n",
    "        for i in range(v):\n",
    "            t.append(k)\n",
    "\n",
    "    return random.choice(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = histogram(['a', 'a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a b a a a a b b a a a "
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    print(choose_from_hist(h), end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3  Word histogram\n",
    "\n",
    "*These are the functions from the book, and the results they would return:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 162742\n",
      "Number of word types: 7460\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def process_file(filename):\n",
    "    hist = dict()\n",
    "    fp = open(filename)\n",
    "    for line in fp:\n",
    "        process_line(line, hist)\n",
    "    return hist\n",
    "\n",
    "def process_line(line, hist):\n",
    "    line = line.replace('-', ' ')\n",
    "    \n",
    "    for word in line.split():\n",
    "        word = word.strip(string.punctuation + string.whitespace)\n",
    "        word = word.lower()\n",
    "        hist[word] = hist.get(word, 0) + 1\n",
    "        \n",
    "hist = process_file('emma.txt')\n",
    "\n",
    "def total_words(hist):\n",
    "    return sum(hist.values())\n",
    "\n",
    "def different_words(hist):\n",
    "    return len(hist)\n",
    "\n",
    "# technical name for 'total words' is 'tokens'\n",
    "print('Total number of tokens:', total_words(hist))\n",
    "\n",
    "# technical name for 'different words' is 'word types'\n",
    "print('Number of word types:', different_words(hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__One problem with the above code is that is doesn't remove the boilerplate at the beginning of the text, and yes, the [text posted on the html version of \"Think Python 2e\"](http://greenteapress.com/thinkpython2/code/emma.txt \"emma.txt\") has boilerplate, at least when I wrote this.  If we download a new version of the plain text format of \"Emma\" and rerun the analysis with the code in this notebook, we'll get different results, as my code will clean out the boilerplate:__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 161989\n",
      "Number of word types: 7097\n"
     ]
    }
   ],
   "source": [
    "clean = clean_pg_text('emma_new.txt')\n",
    "tally = tally_words(clean)\n",
    "\n",
    "# technical name for this is number of tokens\n",
    "print('Total number of tokens:', total_words(tally))\n",
    "\n",
    "# technical name for this is word types\n",
    "print('Number of word types:', different_words(tally))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
